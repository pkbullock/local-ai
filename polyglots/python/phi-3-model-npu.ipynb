{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example generated by GPT 4o model, to run a Phi-3 model\n",
    "\n",
    "The `input_data` should be structured according to what the Phi-3 model expects. If the model is a language model, `input_data` would typically be a tokenized representation of a text prompt.\n",
    "\n",
    "Here's how you might handle text input:\n",
    "\n",
    "1. **Tokenization**: Convert the text prompt into tokens using a tokenizer compatible with the model.\n",
    "\n",
    "2. **Input Format**: Ensure the tokens are in the correct format (e.g., a sequence of integers) and shape expected by the model.\n",
    "\n",
    "Here's an example of how you might modify the script to handle text input:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "def load_model(model_path):\n",
    "    session = ort.InferenceSession(model_path)\n",
    "    return session\n",
    "\n",
    "def prepare_input(prompt, tokenizer):\n",
    "    # Tokenize the input prompt\n",
    "    tokens = tokenizer(prompt, return_tensors='np')\n",
    "    return tokens['input_ids']\n",
    "\n",
    "def run_inference(session, input_data):\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    outputs = session.run(None, {input_name: input_data})\n",
    "    return outputs\n",
    "\n",
    "def main():\n",
    "    model_path = 'path/to/phi-3-model.onnx'\n",
    "    tokenizer = AutoTokenizer.from_pretrained('phi-3-tokenizer')  # Replace with actual tokenizer\n",
    "\n",
    "    session = load_model(model_path)\n",
    "    \n",
    "    prompt = \"Your text prompt here\"\n",
    "    prepared_input = prepare_input(prompt, tokenizer)\n",
    "    \n",
    "    outputs = run_inference(session, prepared_input)\n",
    "    \n",
    "    print(\"Model outputs:\", outputs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "- **Tokenizer**: Use a tokenizer that matches the model's training setup. Replace `'phi-3-tokenizer'` with the appropriate tokenizer identifier.\n",
    "\n",
    "- **Prompt**: The `prompt` variable is where you input your text prompt.\n",
    "\n",
    "- **Tokenization**: The `prepare_input` function uses the tokenizer to convert the text into a format suitable for the model.\n",
    "\n",
    "- **Model Compatibility**: Ensure the tokenizer and model are compatible in terms of vocabulary and tokenization strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PaulBullock\\.conda\\envs\\py311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "def load_model(model_path):\n",
    "    session = ort.InferenceSession(model_path)\n",
    "    return session\n",
    "\n",
    "def prepare_input(prompt, tokenizer):\n",
    "    # Tokenize the input prompt\n",
    "    tokens = tokenizer(prompt, return_tensors='np')\n",
    "    input_ids = tokens['input_ids']\n",
    "    attention_mask = tokens['attention_mask']\n",
    "    \n",
    "    # Initialize past_key_values with zeros\n",
    "    past_key_values = [np.zeros((1, 12, 0, 64), dtype=np.float32) for _ in range(32)]  # Adjust dimensions as needed\n",
    "    \n",
    "    return input_ids, attention_mask, past_key_values\n",
    "\n",
    "def run_inference(session, input_ids, attention_mask, past_key_values):\n",
    "    input_feed = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "    \n",
    "    # Add past_key_values to input_feed\n",
    "    for i, past_key in enumerate(past_key_values):\n",
    "        input_feed[f'past_key_values.{i}.key'] = past_key\n",
    "        input_feed[f'past_key_values.{i}.value'] = past_key\n",
    "    \n",
    "    outputs = session.run(None, input_feed)\n",
    "    return outputs\n",
    "\n",
    "def main():\n",
    "    model_path = 'C:\\\\ai\\\\models\\\\Phi-3.5-mini-instruct-onnx\\\\cpu_and_mobile\\\\cpu-int4-awq-block-128-acc-level-4\\\\phi-3.5-mini-instruct-cpu-int4-awq-block-128-acc-level-4.onnx'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")  # Replace with actual tokenizer\n",
    "\n",
    "    session = load_model(model_path)\n",
    "    \n",
    "    prompt = \"Tell me a joke\"  # Hello World of Prompts\n",
    "    input_ids, attention_mask, past_key_values = prepare_input(prompt, tokenizer)\n",
    "    \n",
    "    outputs = run_inference(session, input_ids, attention_mask, past_key_values)\n",
    "    \n",
    "    print(\"Model outputs:\", outputs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
