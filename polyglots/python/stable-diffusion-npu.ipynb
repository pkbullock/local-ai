{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction - Stable Diffusion Example using Snapdragon Elite X NPU\n",
    "\n",
    "This is using a reference here to understand to get this running, this is Qualcomms work: https://docs.qualcomm.com/bundle/publicresource/topics/80-64748-1/model_execution_windows.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the required libraries and binaries to libs folder \n",
    "import shutil \n",
    "import os\n",
    "\n",
    "execution_ws = os.getcwd()\n",
    "SDK_dir = execution_ws + \"qnn_assets\\\\\" + #\"<Insert path to unzipped QNN SDK here>\"\n",
    "\n",
    "lib_dir = SDK_dir + \"\\\\lib\\\\aarch64-windows-msvc\\\\\"\n",
    "binary = SDK_dir + \"\\\\bin\\\\aarch64-windows-msvc\\qnn-net-run.exe\"\n",
    "skel = SDK_dir + \"\\\\lib\\\\hexagon-v68\\\\unsigned\\libQnnHtpV68Skel.so\"\n",
    "des_dir = execution_ws + \"qnn_assets\\\\QNN_binaries\"\n",
    "\n",
    "# Copy necessary libraries to a common location\n",
    "libs = [\"QnnHtp.dll\", \"QnnHtpNetRunExtensions.dll\", \"QnnHtpPrepare.dll\", \"QnnHtpV68Stub.dll\"]\n",
    "for lib in libs:\n",
    "    shutil.copy(lib_dir+lib, des_dir)\n",
    "    \n",
    "# Copy binary\n",
    "shutil.copy(binary, des_dir)\n",
    "\n",
    "# Copy Skel\n",
    "shutil.copy(skel, des_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Any user defined prompt\n",
    "user_prompt = \"decorated modern country house interior, 8 k, light reflections\"\n",
    "\n",
    "# User defined seed value\n",
    "user_seed = np.int64(1.36477711e+14)\n",
    "\n",
    "# User defined step value, any integer value in {20, 50}\n",
    "user_step = 20\n",
    "\n",
    "# User define text guidance, any float value in [5.0, 15.0]\n",
    "user_text_guidance = 7.5\n",
    "\n",
    "# Error checking for user_seed\n",
    "assert isinstance(user_seed, np.int64) == True,\"user_seed should be of type int64\"\n",
    "\n",
    "# Error checking for user_step\n",
    "assert isinstance(user_step, int) == True,\"user_step should be of type int\"\n",
    "assert user_step == 20 or user_step == 50,\"user_step should be either 20 or 50\"\n",
    "\n",
    "# Error checking for user_text_guidance\n",
    "assert isinstance(user_text_guidance, float) == True,\"user_text_guidance should be of type float\"\n",
    "assert user_text_guidance >= 5.0 and user_text_guidance <= 15.0,\"user_text_guidance should be a float from [5.0, 15.0]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import UNet2DConditionModel\n",
    "from diffusers.models.embeddings import get_timestep_embedding\n",
    "\n",
    "# pre-load time embedding\n",
    "time_embeddings = UNet2DConditionModel.from_pretrained('runwayml/stable-diffusion-v1-5',\n",
    "                                                       subfolder='unet', cache_dir='./cache/diffusers').time_embedding\n",
    "\n",
    "def get_time_embedding(timestep):\n",
    "    timestep = torch.tensor([timestep])\n",
    "    t_emb = get_timestep_embedding(timestep, 320, True, 0)\n",
    "    \n",
    "    emb = time_embeddings(t_emb).detach().numpy()\n",
    "    \n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Define Tokenizer output max length (must be 77)\n",
    "tokenizer_max_length = 77\n",
    "\n",
    "# Initializing the Tokenizer\n",
    "tokenizer = Tokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Setting max length to tokenizer_max_length\n",
    "tokenizer.enable_truncation(tokenizer_max_length)\n",
    "tokenizer.enable_padding(pad_id=49407, length=tokenizer_max_length)\n",
    "\n",
    "def run_tokenizer(prompt):\n",
    "    # Run Tokenizer encoding\n",
    "    token_ids = tokenizer.encode(prompt).ids\n",
    "    # Convert tokens list to np.array\n",
    "    token_ids = np.array(token_ids, dtype=np.float32)\n",
    "\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "\n",
    "# Initializing the Scheduler\n",
    "scheduler = DPMSolverMultistepScheduler(num_train_timesteps=1000, beta_start=0.00085,\n",
    "                                        beta_end=0.012, beta_schedule=\"scaled_linear\")\n",
    "# Setting up user provided time steps for Scheduler\n",
    "scheduler.set_timesteps(user_step)\n",
    "\n",
    "def run_scheduler(noise_pred_uncond, noise_pred_text, latent_in, timestep):\n",
    "    # Convert all inputs from NHWC to NCHW\n",
    "    noise_pred_uncond = np.transpose(noise_pred_uncond, (0,3,1,2)).copy()\n",
    "    noise_pred_text = np.transpose(noise_pred_text, (0,3,1,2)).copy()\n",
    "    latent_in = np.transpose(latent_in, (0,3,1,2)).copy()\n",
    "\n",
    "    # Convert all inputs to torch tensors\n",
    "    noise_pred_uncond = torch.from_numpy(noise_pred_uncond)\n",
    "    noise_pred_text = torch.from_numpy(noise_pred_text)\n",
    "    latent_in = torch.from_numpy(latent_in)\n",
    "\n",
    "    # Merge noise_pred_uncond and noise_pred_text based on user_text_guidance\n",
    "    noise_pred = noise_pred_uncond + user_text_guidance * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # Run Scheduler step\n",
    "    latent_out = scheduler.step(noise_pred, timestep, latent_in).prev_sample.numpy()\n",
    "    \n",
    "    # Convert latent_out from NCHW to NHWC\n",
    "    latent_out = np.transpose(latent_out, (0,2,3,1)).copy()\n",
    "    \n",
    "    return latent_out\n",
    "\n",
    "# Function to get timesteps\n",
    "def get_timestep(step):\n",
    "    return np.int32(scheduler.timesteps.numpy()[step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Tokenizer\n",
    "uncond_tokens = run_tokenizer(\"\")\n",
    "cond_tokens = run_tokenizer(user_prompt)\n",
    "\n",
    "# Run Text Encoder on Tokens\n",
    "uncond_text_embedding = run_text_encoder(uncond_tokens)\n",
    "user_text_embedding = run_text_encoder(cond_tokens)\n",
    "\n",
    "# Initialize the latent input with random initial latent\n",
    "random_init_latent = torch.randn((1, 4, 64, 64), generator=torch.manual_seed(user_seed)).numpy()\n",
    "latent_in = random_init_latent.transpose((0, 2, 3, 1)).copy()\n",
    "\n",
    "# Run the loop for user_step times\n",
    "for step in range(user_step):\n",
    "    print(f'Step {step} Running...')\n",
    "    \n",
    "    # Get timestep from step\n",
    "    timestep = get_timestep(step)\n",
    "\n",
    "    # Run U-net for const embeddings\n",
    "    unconditional_noise_pred = run_unet(latent_in, get_time_embedding(timestep), uncond_text_embedding)\n",
    "\n",
    "    # Run U-net for user text embeddings\n",
    "    conditional_noise_pred = run_unet(latent_in, get_time_embedding(timestep), user_text_embedding)\n",
    "    \n",
    "    # Run Scheduler\n",
    "    latent_in = run_scheduler(unconditional_noise_pred, conditional_noise_pred, latent_in, timestep)\n",
    "\n",
    "# Run VAE\n",
    "output_image = run_vae(latent_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# Display the generated output\n",
    "display(Image.fromarray(output_image, mode=\"RGB\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
