{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction - Learning how to run inference on a Super Resolution Model\n",
    "\n",
    "This is testing the process of running a model on the NPU. It was a bit fiddly bit have this working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Available Onnxruntime providers: ['QNNExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    print(f\"[INFO] Available Onnxruntime providers: {onnxruntime.get_available_providers()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Note: This should list the available providers for the current system specifically looking for QNNExecutionProvider, as this will use the NPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "- Install the Qualcomm AI SDK - https://softwarecenter.qualcomm.com/#/catalog/item/qualcomm_ai_engine_direct?type=Tool\n",
    "- Reinstall the onnxruntime QNN if done before\n",
    "- Ensure that Visual Studio 2022 has C++ dev setup\n",
    "\n",
    "If all fails Qualcomm Slack support is excellent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_provider_option = {\n",
    "    \"backend_path\": f\"C:\\\\ai\\\\qcdll\\\\QnnHtp.dll\",\n",
    "    \"session.enable_htp_fp16_precision\": \"1\",\n",
    "    \"htp_performance_mode\": \"high_performance\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full Code Sample with Super Resolution Model working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available providers: ['QNNExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\n",
      "Output image saved as output_image_notebook.png\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "# Install - pip install onnx onnxruntime onnxruntime-qnn pillow\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert('RGB')  # Convert to grayscale\n",
    "    image = image.resize((128, 128))  # Resize to the required input size\n",
    "    image_data = np.array(image).astype(np.uint8)\n",
    "    image_data = image_data.transpose(2, 0, 1)  # Change data layout from HWC to CHW\n",
    "    image_data = np.expand_dims(image_data, axis=0)  # Add batch dimension\n",
    "    return image_data\n",
    "\n",
    "def postprocess_output(output_data):\n",
    "    # Remove batch dimension and change data layout from CHW to HWC\n",
    "    output_image = output_data.squeeze().transpose(1, 2, 0)\n",
    "    # Convert to uint8\n",
    "    output_image = output_image.astype(np.uint8)\n",
    "    return output_image\n",
    "\n",
    "def main(image_path):\n",
    "    # Check available providers\n",
    "    available_providers = ort.get_available_providers()\n",
    "    print(\"Available providers:\", available_providers)\n",
    "\n",
    "    # Load the ONNX model\n",
    "    model_path = r\"C:\\\\ai\\\\models\\\\quicksrnetlarge_quantized.onnx\"  # Ensure this file exists in the correct path\n",
    "\n",
    "    execution_provider_option = {\n",
    "        \"backend_path\": f\"C:\\\\ai\\\\qcdll\\\\QnnHtp.dll\",\n",
    "        \"session.enable_htp_fp16_precision\": \"1\",\n",
    "        \"htp_performance_mode\": \"high_performance\",\n",
    "    }\n",
    "    \n",
    "    # Use QNNExecutionProvider regardless of the available providers\n",
    "    providers = ['QNNExecutionProvider']\n",
    "    ort_session = ort.InferenceSession(model_path, providers=providers, provider_options=[execution_provider_option])\n",
    "    \n",
    "    # Prepare input data\n",
    "    input_name = ort_session.get_inputs()[0].name\n",
    "    input_data = preprocess_image(image_path)\n",
    "\n",
    "    # Run the model\n",
    "    result = ort_session.run(None, {input_name: input_data})\n",
    "\n",
    "    # Post-process the output\n",
    "    output_image = postprocess_output(result[0])\n",
    "\n",
    "    # Save the output image\n",
    "    output_image = Image.fromarray(output_image)\n",
    "    output_image.save(\"output_image_notebook.png\")\n",
    "    print(\"Output image saved as output_image_notebook.png\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main('example_image_128.jpg')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qai_hub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
